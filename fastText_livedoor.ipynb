{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5988a039946b4508adcb6a7158abf7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ca3b6f70034afbbaa04e4edbcad7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=870), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9ed405d2a249aa9129f6ec4c364bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=870), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2e5629cb164e77ba65222f42331bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=864), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb0492886134604b1e0083f481ace42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=511), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42070b951e8a4ed48e801bf04ebc2540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=870), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0307448ba9254fa0aaf73f3a387450aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=842), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e8d44b8b28479ebaa6b6512729c316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=870), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ead438aff449ada0e29c34f11c477e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=900), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b901edf25fc4271b32859225df577fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=770), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#preprocessing\n",
    "dirlist = [\"dokujo-tsushin\",\"it-life-hack\",\"kaden-channel\",\"livedoor-homme\",\n",
    "           \"movie-enter\",\"peachy\",\"smax\",\"sports-watch\",\"topic-news\"]\n",
    "df = pd.DataFrame(columns=[\"label\",\"class\",\"news\"])\n",
    "for index , i in enumerate(tqdm(dirlist)):\n",
    "    path = \"/home/jovyan/work/livedoor/text/\"+i+\"/*.txt\" \n",
    "    files = glob.glob(path)\n",
    "    files.pop()\n",
    "    for j in tqdm(files):\n",
    "        f = open(j,encoding=\"utf-8\")\n",
    "        data = f.read() \n",
    "        f.close()\n",
    "        t = pd.Series(['__label__'+i,i,\"\".join(data.split(\"\\n\")[3:])],index = df.columns)\n",
    "        df  = df.append(t,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                  __label__topic-news\n",
       "class                                           topic-news\n",
       "news     19日にダルビッシュ有とモデル・紗栄子の離婚成立が報じられたが、同日「5時に夢中！」（東京M...\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d40fbe0e2f449d2b03af92cfff8358c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import MeCab\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import re\n",
    "\n",
    "start = time.time()\n",
    "tokenizer =  MeCab.Tagger(\"-Owakati\")  \n",
    "sentences = []\n",
    "hisaka = []\n",
    "print (\"Parsing sentences from training set...\")\n",
    "\n",
    "# Loop over each news article.\n",
    "for review in tqdm(df[\"news\"]):\n",
    "    try:\n",
    "        # Split a review into parsed sentences.\n",
    "        result = tokenizer.parse(review).replace(\"\\u3000\",\"\").replace(\"\\n\",\"\")\n",
    "        result = re.sub(r'[0123456789０１２３４５６７８９！＠＃＄％＾＆\\-|\\\\＊\\“（）＿■×※⇒—●(：〜＋=)／*&^%$#@!~`){}…\\[\\]\\\"\\'\\”:;<>?＜＞？、。・,./『』【】「」→←○]+', \"\", result)\n",
    "        h = result.split(\" \")\n",
    "        h = list(filter((\"\").__ne__, h))\n",
    "        sentences.append(result)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "df['news'] = sentences\n",
    "df.to_csv(\"/home/jovyan/work/livedoor/text/livedoor_fastText2.csv\",columns=['label', 'news'],index=False,header=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__dokujo-tsushin'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-17 05:38:44,335 : INFO : collecting all words and their counts\n",
      "2018-12-17 05:38:44,337 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-17 05:38:45,457 : INFO : collected 74372 word types from a corpus of 3917070 raw words and 7367 sentences\n",
      "2018-12-17 05:38:45,457 : INFO : Loading a fresh vocabulary\n",
      "2018-12-17 05:38:45,745 : INFO : effective_min_count=1 retains 74372 unique words (100% of original 74372, drops 0)\n",
      "2018-12-17 05:38:45,746 : INFO : effective_min_count=1 leaves 3917070 word corpus (100% of original 3917070, drops 0)\n",
      "2018-12-17 05:38:46,099 : INFO : deleting the raw counts dictionary of 74372 items\n",
      "2018-12-17 05:38:46,100 : INFO : sample=0.001 downsamples 33 most-common words\n",
      "2018-12-17 05:38:46,101 : INFO : downsampling leaves estimated 2902050 word corpus (74.1% of prior 3917070)\n",
      "2018-12-17 05:38:47,189 : INFO : estimated required memory for 74372 words, 446339 buckets and 4 dimensions: 58928448 bytes\n",
      "2018-12-17 05:38:47,207 : INFO : resetting layer weights\n",
      "2018-12-17 05:38:49,896 : INFO : Total number of ngrams is 446339\n",
      "2018-12-17 05:38:52,671 : INFO : training model with 3 workers on 74372 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2018-12-17 05:38:53,680 : INFO : EPOCH 1 - PROGRESS: at 27.22% examples, 892376 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:38:54,681 : INFO : EPOCH 1 - PROGRESS: at 54.58% examples, 865117 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-17 05:38:55,685 : INFO : EPOCH 1 - PROGRESS: at 85.50% examples, 880400 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:38:55,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:38:55,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:38:55,992 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:38:55,993 : INFO : EPOCH - 1 : training on 3917070 raw words (2902667 effective words) took 3.3s, 874588 effective words/s\n",
      "2018-12-17 05:38:56,996 : INFO : EPOCH 2 - PROGRESS: at 26.81% examples, 889076 words/s, in_qsize 1, out_qsize 0\n",
      "2018-12-17 05:38:57,997 : INFO : EPOCH 2 - PROGRESS: at 54.83% examples, 870599 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:38:59,004 : INFO : EPOCH 2 - PROGRESS: at 85.50% examples, 880613 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:38:59,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:38:59,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:38:59,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:38:59,305 : INFO : EPOCH - 2 : training on 3917070 raw words (2901561 effective words) took 3.3s, 876565 effective words/s\n",
      "2018-12-17 05:39:00,314 : INFO : EPOCH 3 - PROGRESS: at 27.22% examples, 891606 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:01,324 : INFO : EPOCH 3 - PROGRESS: at 54.58% examples, 860258 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-17 05:39:02,332 : INFO : EPOCH 3 - PROGRESS: at 84.66% examples, 871201 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:02,646 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:02,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:02,654 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:02,654 : INFO : EPOCH - 3 : training on 3917070 raw words (2901200 effective words) took 3.3s, 866724 effective words/s\n",
      "2018-12-17 05:39:03,661 : INFO : EPOCH 4 - PROGRESS: at 24.42% examples, 842423 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:04,664 : INFO : EPOCH 4 - PROGRESS: at 53.28% examples, 843564 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:05,677 : INFO : EPOCH 4 - PROGRESS: at 82.99% examples, 863105 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:06,021 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:06,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:06,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:06,030 : INFO : EPOCH - 4 : training on 3917070 raw words (2901598 effective words) took 3.4s, 860133 effective words/s\n",
      "2018-12-17 05:39:07,037 : INFO : EPOCH 5 - PROGRESS: at 23.59% examples, 830644 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:08,048 : INFO : EPOCH 5 - PROGRESS: at 53.09% examples, 838030 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-17 05:39:09,056 : INFO : EPOCH 5 - PROGRESS: at 80.47% examples, 849111 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:09,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:09,483 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:09,491 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:09,492 : INFO : EPOCH - 5 : training on 3917070 raw words (2903116 effective words) took 3.5s, 839572 effective words/s\n",
      "2018-12-17 05:39:10,504 : INFO : EPOCH 6 - PROGRESS: at 25.64% examples, 862921 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:11,513 : INFO : EPOCH 6 - PROGRESS: at 52.30% examples, 822774 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:12,519 : INFO : EPOCH 6 - PROGRESS: at 76.95% examples, 827411 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:13,058 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:13,060 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:13,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:13,067 : INFO : EPOCH - 6 : training on 3917070 raw words (2901874 effective words) took 3.6s, 812992 effective words/s\n",
      "2018-12-17 05:39:14,078 : INFO : EPOCH 7 - PROGRESS: at 27.61% examples, 898631 words/s, in_qsize 1, out_qsize 0\n",
      "2018-12-17 05:39:15,083 : INFO : EPOCH 7 - PROGRESS: at 54.83% examples, 865765 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:16,092 : INFO : EPOCH 7 - PROGRESS: at 84.66% examples, 872221 words/s, in_qsize 1, out_qsize 0\n",
      "2018-12-17 05:39:16,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:16,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:16,412 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:16,413 : INFO : EPOCH - 7 : training on 3917070 raw words (2902459 effective words) took 3.3s, 868100 effective words/s\n",
      "2018-12-17 05:39:17,418 : INFO : EPOCH 8 - PROGRESS: at 23.94% examples, 838776 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:18,419 : INFO : EPOCH 8 - PROGRESS: at 49.56% examples, 782651 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:19,422 : INFO : EPOCH 8 - PROGRESS: at 76.35% examples, 824932 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:19,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:19,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:19,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:19,935 : INFO : EPOCH - 8 : training on 3917070 raw words (2902024 effective words) took 3.5s, 825037 effective words/s\n",
      "2018-12-17 05:39:20,941 : INFO : EPOCH 9 - PROGRESS: at 27.22% examples, 895011 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:21,948 : INFO : EPOCH 9 - PROGRESS: at 54.83% examples, 867227 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:22,956 : INFO : EPOCH 9 - PROGRESS: at 85.50% examples, 877937 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:23,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:23,240 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:23,246 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:23,247 : INFO : EPOCH - 9 : training on 3917070 raw words (2903135 effective words) took 3.3s, 876910 effective words/s\n",
      "2018-12-17 05:39:24,260 : INFO : EPOCH 10 - PROGRESS: at 26.81% examples, 882018 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:25,262 : INFO : EPOCH 10 - PROGRESS: at 54.83% examples, 866735 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:26,266 : INFO : EPOCH 10 - PROGRESS: at 85.08% examples, 876822 words/s, in_qsize 0, out_qsize 0\n",
      "2018-12-17 05:39:26,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-17 05:39:26,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-17 05:39:26,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-17 05:39:26,576 : INFO : EPOCH - 10 : training on 3917070 raw words (2902514 effective words) took 3.3s, 872867 effective words/s\n",
      "2018-12-17 05:39:26,576 : INFO : training on a 39170700 raw words (29022148 effective words) took 33.9s, 856004 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pandas as pd\n",
    "\n",
    "sample = LineSentence(\"/home/jovyan/work/livedoor/text/livedoor_fastText2.csv\")\n",
    "model = FastText(sample, size=4, window=3, min_count=1, iter=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-17 05:39:44,275 : INFO : saving FastText object under /tmp/fasttext.model, separately None\n",
      "2018-12-17 05:39:44,276 : INFO : not storing attribute vectors_norm\n",
      "2018-12-17 05:39:44,278 : INFO : not storing attribute vectors_vocab_norm\n",
      "2018-12-17 05:39:44,279 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2018-12-17 05:39:44,280 : INFO : not storing attribute buckets_word\n",
      "2018-12-17 05:39:44,653 : INFO : saved /tmp/fasttext.model\n",
      "2018-12-17 05:39:44,655 : INFO : loading FastText object from /tmp/fasttext.model\n",
      "2018-12-17 05:39:45,013 : INFO : loading wv recursively from /tmp/fasttext.model.wv.* with mmap=None\n",
      "2018-12-17 05:39:45,015 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-12-17 05:39:45,015 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2018-12-17 05:39:45,016 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2018-12-17 05:39:45,016 : INFO : setting ignored attribute buckets_word to None\n",
      "2018-12-17 05:39:45,017 : INFO : loading vocabulary recursively from /tmp/fasttext.model.vocabulary.* with mmap=None\n",
      "2018-12-17 05:39:45,018 : INFO : loading trainables recursively from /tmp/fasttext.model.trainables.* with mmap=None\n",
      "2018-12-17 05:39:45,018 : INFO : loaded /tmp/fasttext.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"fasttext.model\")\n",
    "model.save(fname)\n",
    "model = FastText.load(fname)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existent_word = \"事件\"\n",
    "existent_word in model.wv.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_matching = model.wv.doesnt_match(\"human computer interface tree\".split())\n",
    "sim_score = model.wv.similarity('computer', 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interface'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98126632"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('師範', 0.9993112087249756)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = model.wv.most_similar(positive=['computer', 'human'], negative=['interface'])\n",
    "most_similar = similarities[0]\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
